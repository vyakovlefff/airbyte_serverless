# Define the Airbyte connection name
connection_name: "airbyte_serverless_connection"

# Define the source connector (Example: Fake Data Generator)
source:
  docker_image: "airbyte/source-faker:latest"
  config:
    count: 100  # Number of records to generate
    seed: 42    # Seed for reproducibility

# Define the destination connector (Example: Google BigQuery)
destination:
  docker_image: "airbyte/destination-bigquery:latest"
  config:
    project_id: "my-gcp-project"
    dataset_id: "airbyte_dataset"
    credentials_json: 
      type: "service_account"
      project_id: "my-gcp-project"
      private_key_id: "YOUR_PRIVATE_KEY_ID"
      private_key: "-----BEGIN PRIVATE KEY-----\nYOUR_PRIVATE_KEY\n-----END PRIVATE KEY-----\n"
      client_email: "your-service-account@your-project.iam.gserviceaccount.com"
      client_id: "YOUR_CLIENT_ID"
      auth_uri: "https://accounts.google.com/o/oauth2/auth"
      token_uri: "https://oauth2.googleapis.com/token"
      auth_provider_x509_cert_url: "https://www.googleapis.com/oauth2/v1/certs"
      client_x509_cert_url: "YOUR_CERT_URL"

# Define the sync settings
sync:
  frequency: "manual"  # Can also be a cron expression
  namespace_format: "my_airbyte_namespace"
  streams:
    - name: "users"
      sync_mode: "full_refresh"
      destination_sync_mode: "append"
      primary_key: ["id"]
      cursor_field: ["updated_at"]

# Extra configuration to avoid missing variable errors
runtime:
  entrypoint: "run-env-vars"
  yaml_config: "/app/connections/config.yaml"
