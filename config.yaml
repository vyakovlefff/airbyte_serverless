# Define the Airbyte connection name
connection_name: "airbyte_serverless_connection"

# Define the source connector (Example: Fake Data Generator)
source:
  docker_image: "airbyte/source-faker:latest"
  config:
    count: 100  # Number of records to generate
    seed: 42    # Seed for reproducibility

# Define the destination connector (Example: Google BigQuery)
destination:
  docker_image: "airbyte/destination-bigquery:latest"
  config:
    project_id: "my-gcp-project"
    dataset_id: "airbyte_dataset"
    credentials_json: GCP_SECRET("projects/my-gcp-project/secrets/bigquery-key/versions/latest")  # Uses Google Secret Manager for security

# Define the sync settings
sync:
  frequency: "manual"  # Can also be a cron expression
  namespace_format: "my_airbyte_namespace"
  streams:
    - name: "users"
      sync_mode: "full_refresh"
      destination_sync_mode: "append"
      primary_key: ["id"]
      cursor_field: ["updated_at"]

# Remote runner for Cloud Run
remote_runner:
  type: "cloud_run_job"
  service_account: "your-service-account@your-project.iam.gserviceaccount.com"

# Extra configuration to avoid missing variable errors
runtime:
  entrypoint: "run-env-vars"
  yaml_config: "/app/connections/config.yaml"
